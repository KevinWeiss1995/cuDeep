# .cursorrules
project:
  name: "cuDeep"
  type: "python-cuda-library"
  description: "Ultra-high performance deep learning library implemented in low-level CUDA, Python bindings included"
  version: "0.1.0"
  repo: "git@github.com:username/cuInferno.git"
  license: "MIT"
  maintainers:
    - "Kevin <youremail@gwu.edu>"

structure:
  root:
    - src/
    - include/
    - python/
    - tests/
    - benchmarks/
    - docs/
    - examples/
    - CMakeLists.txt
    - setup.py
    - .gitignore
    - README.md

cuda:
  min_version: "12.2"
  max_version: "13.x"
  code_conventions:
    - "All kernels must be templated for data type flexibility (float16, float32, float64)"
    - "Use warp-level primitives where possible to avoid shared memory contention"
    - "No unnecessary host<->device transfers in critical paths"
    - "Prefer asynchronous streams with event synchronization for overlapping compute + memory transfers"
    - "Every kernel must be benchmarked against equivalent cuBLAS/cuDNN ops"
  performance:
    target_ops_per_sec: ">= PyTorch cuDNN ops by 20%"
    memory_footprint_optimization: true
    tensor_layout: "NCHW preferred, NHWC optional if faster on Ampere+"
    fused_ops: true
  debugging:
    tools: ["Nsight Compute", "Nsight Systems", "cuda-memcheck"]
    assertions: "All kernel launches must check cudaGetLastError()"

python_bindings:
  method: "PyBind11"
  type_annotations: true
  conventions:
    - "All public APIs must be snake_case"
    - "Modules: tensor.py, layers.py, optim.py, nn.py, utils.py"
    - "Support standard NumPy interoperability (from_numpy, to_numpy)"
    - "Automatic dispatch to best CUDA kernel based on input dtype + device"
  packaging:
    pip_installable: true
    wheels: "linux-x86_64-cuda, linux-aarch64-cuda"
    CI_tests: true

testing:
  unit_tests: true
  integration_tests: true
  performance_tests:
    - "Matrix Multiplication: compare against cuBLAS"
    - "Convolutions: compare against cuDNN"
    - "Backprop: verify gradients numerically"
  framework: "pytest + custom benchmark scripts"
  coverage_threshold: 90%

documentation:
  format: "Markdown + Sphinx"
  examples: true
  API_reference: true
  tutorials: true
  performance_tips: true

ci_cd:
  github_actions: true
  checks:
    - "Linting: clang-format for CUDA, black for Python"
    - "Unit tests + coverage"
    - "Benchmark reproducibility"
    - "Static analysis: cppcheck / clang-tidy"
  nightly_builds: true

naming_conventions:
  cuda_kernels: "snake_case + _kernel suffix"
  device_functions: "snake_case + _device suffix"
  host_functions: "snake_case"
  classes: "CamelCase"
  tensors: "Tensor<dtype, layout>"

performance_goals:
  throughput: "â‰¥1.2x PyTorch baseline on same GPU"
  latency: "Minimize host-device round trips"
  memory: "In-place ops preferred, reduce allocations"
  parallelism: "Use streams, warps, and cooperative groups aggressively"

optional_features:
  - mixed_precision: true
  - tensor_cores: true
  - fused_activation: true
  - custom_autograd: true

# End of cursorrules